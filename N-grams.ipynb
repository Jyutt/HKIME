{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated on Wed Dec 25 03:49:17 2019 UTC\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(f\"Last updated on {time.asctime(time.gmtime())} UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "\n",
    "This notebook would serve to show the design process of the Back-off n-grams Language Models (BNLM) with enhancement from Neural Network Language Models (NNLM) as described the paper \"Neural Network Language Model for Chinese Pinyin Input Method Engine\" by Chen et all. The task at hand with this language model is given a sequence of syllables, to predict which is most likely the next syllable, a task also known as candidate sentence generation. The model is to be implemented into HKIME, an intelligent input method for Cantonese. There would be three sections in this notebook, each building upon the previous. \n",
    "\n",
    "### Breakdown\n",
    "\n",
    "- Section 1: Basic n-grams probabilistic particle filter\n",
    "- Section 2: Back-off n-grams language model with interpolated Kneser-Ney smoothing\n",
    "- Section 3: BNLM (from section 2) with probabilities calculated with NNLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jyutping Corpus Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['海港城於周二平安夜發生一連串警民衝突，最少兩名市民被捕，一名市民路經時被警方無理用警棍打頭而要送院。', '衝突發生過後，現場除了有市民和防暴警員外，佐敦南選區候任區議員陳梓維亦在場。', '他不單在場用手機進行直播，告知街坊海港城的最新情況，更表示希望他在場能 唔好畀佢 警方 亂咁濫用暴力 。', '', '', '陳梓維原本受邀在平安夜晚上到一間教會參與活動，但當他得知海港城內有衝突發生，便決定中途離開，趕到海港城現場；到場時，港威商場三樓已全層封閉，二樓亦有防暴警員戒備。', '同時，他亦見到該名較早前頭部受傷的市民在現場接受救護員治療。', '進入海港城前，陳梓維已在專頁 開 ；到場後，他眼見現場環境頗為混亂，遂決定繼續直播，向街坊進行現場報道。', '', '', '訪問途中，陳梓維提到一個小插曲，指他到場後，有一名防暴警員認得他為候任區議員，要求他 控制、處理現場環境 。', '陳梓維表示，警員當時的一番說話 即時引起全場哄動 。', '最後，該名警員在現場市民的叫嚷聲中，退至商場出入口處。', '作為候任區議員，陳梓維認為自己在場，可以 唔好畀佢 警方 亂咁濫用暴力 ，並且表示即使警方要進行拘捕，但都一定不可以使用過份武力。', '', '', '最後，記者問陳梓維在平安夜有甚麼說話想對香港人說？', '陳梓維不禁一笑，並表示希望每位香港人 平安夜能夠安全回家 ，並且能開心快樂地度過這個節日。', '', '', '當訪問完結後，又有警員走前並手指陳梓維說，這就是 會考零分都做到區議員嗰個人 。', '諷刺的是，那位警員在揶揄陳梓維的會考分數，卻不知道自己錯把陳梓維名字中的 梓  正確讀音 趾 誤讀為 辛 ，令在進行手機直播中的陳梓維，都忍不住當場糾正，令對方尷尬非常。 ']\n"
     ]
    }
   ],
   "source": [
    "#TODO: Add corpus processing\n",
    "#TODO: Find better corpuses\n",
    "#TODO: Look into webscraping to generate corpuses ourselves\n",
    "\n",
    "FILENAME = \"sources/lihkg_corpus\"\n",
    "\n",
    "with open(FILENAME, \"r\") as f:\n",
    "    content = f.read()\n",
    "    jyutping_corpus = content.splitlines()\n",
    "print(jyutping_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: basic n-grams prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-processing of the cantonese corpus would get us a list of strings, where each string could be a phrase, a sentence, or a paragraph. For conciseness, we would call all of these sentences. In this section, we would divide up each sentence into the n-grams and then store the possible next letters for each n-gram in a python dictionary. The naive prediction algorithm would randomly pick from the possible next letters given a certain n-gram to generate candidate sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set n-grams character count (the n in n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_COUNT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating n-grams dictionary\n",
    "\n",
    "This would generate a dictionary where each key is an n-gram and the value would be a list of possible next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dictionary for prediction\n",
    "def generate_n_grams_dict(processed_corpus):\n",
    "    result = dict()\n",
    "    for sentence in processed_corpus:\n",
    "        #i is the start index of the slice\n",
    "        for i in range(len(sentence) - CHARACTER_COUNT - 1): #-1 since last slice does not have next char\n",
    "            grams = sentence[i:i+CHARACTER_COUNT]\n",
    "            next_char = sentence[i+CHARACTER_COUNT]\n",
    "            if grams in result:\n",
    "                result[grams].append(next_char)\n",
    "            else:\n",
    "                #as long as there is an n-gram key in the dict, there would be at least one next char\n",
    "                result[grams] = [next_char] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: visualization of n-grams dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Model\n",
    "\n",
    "This naive prediction model would, if the sentence has an n-gram in the dictionary, randomly select a next character from the list of potential next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a next character given an n_grams_dict and a sentence.\n",
    "def predict_next_char(n_grams_dict, sentence):\n",
    "    potentials = n_grams_dict.get(sentence[-CHARACTER_COUNT:], None)\n",
    "    return random.choice(potentials) if potentials != None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Here we would test the implementation of the naive n-grams prediction model, for comparison with more sophisticated language models. We would do two tests, the first one would generate a 200 character sentence, and the second would test the implementation analytically by seeing how many next characters it will predict correctly on the test dataset.\n",
    "\n",
    "TODO: Add a validation dataset. The current corpus is too small to be used both for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: \n",
      "最後，該名警員走前並手指陳梓維不禁一笑，並且表示希望他在場用手機進行拘捕，但當他得知海港城內有衝突發生過後，記者問陳梓維已在專頁 開 ；到場時，港威商場出入口處\n",
      "Trial 2: \n",
      "最後，他眼見現場除了有市民和防暴警員當時的一番說話想對香港人 平安夜有甚麼說話 即時引起全場哄動 \n"
     ]
    }
   ],
   "source": [
    "def testing(sentence):\n",
    "    n_grams_dict = generate_n_grams_dict(jyutping_corpus)\n",
    "    tmp = sentence\n",
    "    #Generate a sentence of up to 200 characters, will break if an n-gram not found in n-grams dict.\n",
    "    for i in range(200):\n",
    "        res = predict_next_char(n_grams_dict, tmp)\n",
    "        if res == None:\n",
    "            break\n",
    "        else:\n",
    "            tmp = tmp + res\n",
    "    return tmp\n",
    "\n",
    "# Observe that due to the stochastic nature of\n",
    "# particle filtering, the result is not\n",
    "# pre-determinable\n",
    "print(\"Trial 1: \")\n",
    "print(testing(\"最後\"))\n",
    "print(\"Trial 2: \")\n",
    "print(testing(\"最後\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2\n",
    "**WARNING** Not a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 629 predictions made\n",
      "552 predictions correct\n",
      "Prediction accuracy: 0.8761904761904762%\n"
     ]
    }
   ],
   "source": [
    "n_grams_dict = generate_n_grams_dict(jyutping_corpus)\n",
    "count = 0\n",
    "correct = 0\n",
    "for sentence in jyutping_corpus:\n",
    "    for i in range(len(sentence) - CHARACTER_COUNT - 1):\n",
    "        if predict_next_char(n_grams_dict, sentence[:i+CHARACTER_COUNT]) == sentence[i+CHARACTER_COUNT]:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "\n",
    "print(f\"Total of {count} predictions made\")\n",
    "print(f\"{correct} predictions correct\")\n",
    "print(f\"Prediction accuracy: {correct/(count+1)}%\") #TODO: Remove the +1 after there is content in the jyutping_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Back-off n-grams language model with interpolated Kneser-Ney smoothing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
